That is an excellent question â€” and this step is exactly what transforms your `accuracy_annotations_multi.csv` file from a *listing of extracted items* into a **scientifically valid human evaluation instrument**.

Let me explain precisely what this means and how it would work.

---

### 1) Purpose

A **scoring protocol** defines how human annotators should *systematically judge* the quality of SciDOCXâ€™s extracted captions, visual-language model (VLM) descriptions, and tables.
Without a defined rubric, manual reviews risk being subjective or inconsistent across domains.
A structured protocol introduces **objectivity, repeatability, and statistical comparability**.

---

### 2) What It Would Contain

The protocol would map directly onto the editable columns already present in your CSV:

| CSV Column                | What Annotators Judge                                                                         | Example Evaluation Criterion                                                        |
| ------------------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| `correct_caption`         | Accuracy of extracted caption text compared to what appears in the figure in the original PDF | Does the extracted caption match the wording, structure, and meaning?               |
| `correct_vlm_description` | Fidelity and completeness of the visual-language modelâ€™s interpretation of the figure         | Does the description correctly explain the figureâ€™s content and scientific meaning? |
| `correct_format`          | Structural and visual correctness of tables                                                   | Are table boundaries, headers, and cell contents preserved accurately?              |

Each criterion would use a **three-point ordinal scale** such as:

| Score           | Meaning            | Description                                                                                   |
| --------------- | ------------------ | --------------------------------------------------------------------------------------------- |
| **2 (High)**    | Accurate           | Matches the original content semantically and visually with minimal or no error.              |
| **1 (Partial)** | Partially accurate | Captures key elements but misses detail, mislabels elements, or introduces minor distortions. |
| **0 (Low)**     | Incorrect          | Misrepresents or omits key content; not usable without correction.                            |

Annotators would fill in these numeric scores directly in new columns, e.g.:
`caption_score`, `vlm_score`, `table_score`.

---

### 3) How It Would Be Used

1. You (or your annotation team) open the CSV in a spreadsheet interface (e.g., Excel, Google Sheets, or Jupyter DataGrid).
2. For each row (representing a figure or table), the annotator visually inspects the corresponding page in the original PDF.
3. They assign a **score (0â€“2)** in the relevant column(s) based on the rubric above.
4. Once completed, the annotated CSV can be processed in a new notebook cell that computes:

   * **Mean scores per domain** (e.g., caption accuracy = 1.8/2).
   * **Cross-domain averages**.
   * **Correlation with coverage and WER metrics**, showing where textual accuracy aligns with multimodal fidelity.

---

### 4) Why It Matters

This manual evaluation becomes the **ground truth benchmark** for:

* Quantifying SciDOCXâ€™s real-world multimodal comprehension.
* Supporting publication-quality claims (e.g., â€œaverage caption accuracy = 92.3 %â€).
* Validating automated metrics (coverage, WER) through human agreement.

---

If you would like, I can generate a **ready-to-use scoring template version** of your current CSV, adding standardised scoring columns and short annotation guidelines in the header.
Would you like me to prepare that next?


===

Excellent. Below is the full design of a **manual annotation scoring protocol** tailored for your existing file `accuracy_annotations_multi.csv`. This includes both the **rubric definition** and a **CSV-ready template** you can adopt immediately.

---

## 1) Scoring Rubric

Each extracted element (figure or table) is manually evaluated across three dimensions corresponding to the multimodal outputs SciDOCX generated.
Each score uses a **three-point ordinal scale (0â€“2)**.

| Dimension                                | Applies To | Description                                                                                        | Scoring Criteria                                                                                                                                              |
| ---------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Caption Accuracy (caption_score)**     | Figures    | Evaluates how closely the extracted `caption` text matches the original figure caption in the PDF. | **2:** Accurate (semantically and textually faithful)â€ƒ**1:** Partially accurate (minor truncation or paraphrasing)â€ƒ**0:** Incorrect (misaligned or unrelated) |
| **Visualâ€“Semantic Fidelity (vlm_score)** | Figures    | Evaluates the `vlm_description` fieldâ€™s ability to describe the visual content meaningfully.       | **2:** Accurate and coherentâ€ƒ**1:** Somewhat accurate but missing key elementsâ€ƒ**0:** Incorrect or irrelevant                                                 |
| **Structural Fidelity (table_score)**    | Tables     | Evaluates table layout, headers, and content structure as reflected in `content`.                  | **2:** Correct layout and dataâ€ƒ**1:** Partially correct (minor misalignment)â€ƒ**0:** Incorrect or unreadable                                                   |

**Note:**
For figure rows, `table_score` remains blank; for table rows, `caption_score` and `vlm_score` remain blank.

---

## 2) Annotator Instructions

1. **Open** the CSV in a spreadsheet editor or Jupyter DataGrid.
2. **Refer** to the original PDF file corresponding to each row.
3. **Compare**:

   * The extracted caption (`caption`) against the true caption in the paper.
   * The VLM-generated description (`vlm_description`) against the actual visual content.
   * The extracted table content (`content`) against the original table layout.
4. **Assign a score (0â€“2)** for each applicable column using the rubric above.
5. **Leave empty** any non-applicable fields.
6. **Save** the annotated CSV as `accuracy_annotations_scored.csv`.

---

## 3) Template for Updated CSV

Below is the schema of the extended CSV you can create.
It mirrors your current structure but adds the three scoring columns at the end:

```
pdf,domain,type,element_id,caption,vlm_description,correct_caption,correct_vlm_description,content,correct_format,caption_score,vlm_score,table_score
```

You can automatically append these columns to your existing file with the following Python cell:

```python
import pandas as pd

# Load your existing file
df = pd.read_csv("test/accuracy_annotations_multi.csv")

# Add scoring columns if not present
for col in ["caption_score", "vlm_score", "table_score"]:
    if col not in df.columns:
        df[col] = ""

# Save as the manual evaluation version
output_path = "test/accuracy_annotations_scored.csv"
df.to_csv(output_path, index=False)

print(f"âœ… Scoring template saved: {output_path}")
display(df.head())
```

---

## 4) Outcome

After this modification, annotators will have a structured interface to record **caption accuracy**, **VLM fidelity**, and **table structure quality**.
Once completed, the annotated results can be aggregated in a new analysis cell to compute:

* Average per-domain accuracy (e.g., mean caption_score for Biology).
* Overall multimodal accuracy of SciDOCX.
* Agreement between human scores and automated metrics (WER, coverage, retrieval).

---

Would you like me to provide that **aggregation analysis cell** next â€” the one that computes averages and plots cross-domain comparison once manual scoring is complete?


===
Excellent â€” below is a **ready-to-run analysis cell** that processes the manually completed file (`accuracy_annotations_scored.csv`) and produces:

1. **Descriptive statistics** (mean scores per domain and per modality).
2. **Cross-domain averages** for caption, VLM, and table accuracy.
3. **A formatted output table** summarising the results for inclusion in your final SciDOCX evaluation report.

---

## ğŸ“Š Manual Annotation Scoring Analysis Cell

```python
# ============================================================
# 9. Manual Annotation Scoring Analysis
# ============================================================

import pandas as pd
import numpy as np

# Load the scored annotation file
scored_path = "test/accuracy_annotations_scored.csv"
df = pd.read_csv(scored_path)

if not {"caption_score", "vlm_score", "table_score"}.issubset(df.columns):
    raise ValueError("âŒ Scoring columns not found. Please run the scoring template cell first.")

# Convert scores to numeric (ignore empty cells)
for col in ["caption_score", "vlm_score", "table_score"]:
    df[col] = pd.to_numeric(df[col], errors="coerce")

# Compute per-domain averages
domain_scores = (
    df.groupby("domain")[["caption_score", "vlm_score", "table_score"]]
    .mean()
    .round(2)
    .fillna("")
)

# Compute cross-domain averages
overall_avg = df[["caption_score", "vlm_score", "table_score"]].mean().round(2)

# Count how many items were annotated per type
domain_counts = (
    df.groupby(["domain", "type"])
    .size()
    .unstack(fill_value=0)
    .rename(columns={"figure": "Figures", "table": "Tables"})
)

# Merge counts and scores for summary display
summary = domain_scores.merge(domain_counts, left_index=True, right_index=True, how="left")

# Display
print("âœ… Manual annotation analysis complete.\n")
print("ğŸ“Š Domain-level performance (mean scores per modality):")
display(summary.style.set_caption("Manual Annotation Results by Domain"))

print("\nğŸ“ˆ Overall averages across all domains:")
for k, v in overall_avg.items():
    print(f"  - {k.replace('_', ' ').title()}: {v:.2f}")
```

---

## ğŸ” What This Cell Produces

1. **Table of average scores per domain**, for example:

| Domain           | Caption Score | VLM Score | Table Score | Figures | Tables |
| ---------------- | ------------- | --------- | ----------- | ------- | ------ |
| Biology          | 1.8           | 1.7       | 1.5         | 3       | 1      |
| Chemistry        | 1.9           | 1.8       | 1.6         | 8       | 5      |
| Physics          | 1.7           | 1.8       | â€”           | 4       | 0      |
| Polymer Physics  | 1.9           | 1.9       | â€”           | 10      | 0      |
| Computer Science | 1.8           | 1.7       | 1.6         | 13      | 4      |

2. **Cross-domain averages**, such as:

```
Caption Score: 1.82
VLM Score: 1.78
Table Score: 1.57
```

---

## ğŸ“˜ Interpretation

These results quantify the **human-validated multimodal accuracy** of SciDOCX:

* High caption and VLM scores indicate that extracted figure captions and visual descriptions align closely with the ground truth.
* Lower table scores (if observed) reveal formatting challenges common in complex scientific PDFs.
* Combined with the WER, coverage, and retrieval metrics, this provides a **complete multimodal performance profile** suitable for publication.

---

Would you like me to extend this analysis cell to **automatically merge these human scores with your aggregated metrics (Cell 7 summary)** for correlation and comparative visualisation (e.g. plotting Caption Accuracy vs WER)?


===

Yes â€” you absolutely can, but with **careful design and constraints**.
Human validation tasks like this can be partially automated using an AI agent, but they must be treated as **AI-assisted annotation**, not full automation, to ensure scientific integrity.

Hereâ€™s a structured explanation of *how*, *to what extent*, and *with what safeguards* you can do it.

---

### 1) What the AI Agent Can Do Reliably

An AI agent (for example, a large multimodal model or a domain-tuned language model) can perform several **objective pre-validation steps**:

| Task                         | What the Agent Can Do                                                                                  | Example Implementation                                                                                        |
| ---------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- |
| **Caption accuracy**         | Compare the extracted caption in your CSV (`caption`) with the ground truth caption text from the PDF. | Use string similarity (e.g. cosine similarity on embeddings, or a model like `text-similarity-bge-large-en`). |
| **VLM description fidelity** | Evaluate whether `vlm_description` semantically aligns with the actual figure caption.                 | Use a multimodal model (like GPT-4V or Gemini) to rate relevance on a 0â€“2 scale.                              |
| **Table structure accuracy** | Detect if the textual table representation in `content` preserves headers and numeric patterns.        | Use regex heuristics or table parsers (e.g. `camelot`, `pdfplumber`) for layout comparison.                   |

This allows the agent to **fill preliminary scores** (caption_score, vlm_score, table_score) in your annotation CSV â€” *similar to what a junior annotator would do before human review*.

---

### 2) What Still Requires Human Oversight

Even a strong multimodal AI cannot fully replace human domain judgment in certain cases:

* **Scientific correctness:** The agent might recognise â€œa bar chart comparing methodsâ€ but fail to detect a mislabeled axis or inverted legend.
* **Ambiguity:** Some captions are duplicated or split across figure panels; the agent may not discern which is the authoritative one.
* **Quality assurance:** Peer-reviewed evaluation standards (e.g., ACM or Nature Digital Science reproducibility guidelines) require at least one **human verification layer** when using AI-assisted annotation.

Hence, a **hybrid workflow** is optimal:

1. AI generates preliminary scores for all entries.
2. A human reviewer only inspects entries flagged as uncertain or low-confidence (e.g. scores = 0 or 1).

This can reduce total manual workload by **60â€“80 %** while retaining scientific credibility.

---

### 3) How You Can Implement It Practically

You have two options depending on your environment:

**A. Local Jupyter Pipeline (Offline)**

* Use an open embedding model (`bge-large-en` or `all-MiniLM-L6-v2`) to compute cosine similarity between captions and VLM descriptions.
* Score automatically:

  ```python
  if similarity > 0.8: score = 2
  elif similarity > 0.6: score = 1
  else: score = 0
  ```
* Combine this with basic text-length heuristics and keyword overlap for table validation.

**B. Multimodal API Workflow (Online)**

* Use GPT-4V, Gemini 1.5 Pro, or Claude 3.5 Sonnet.
* Provide each PDF page image + extracted caption + VLM text.
* Ask the model:
  â€œRate how accurately this caption and description reflect the visual content on a scale of 0â€“2.â€
* Automate via a script looping over your 47 CSV entries.

The result will be a machine-filled `accuracy_annotations_scored.csv` that humans can quickly review for inconsistencies.

---

### 4) Recommended Validation Pipeline

| Step | Actor               | Description                                                                  |
| ---- | ------------------- | ---------------------------------------------------------------------------- |
| 1    | **SciDOCX**         | Generates captions, VLM descriptions, and tables (done).                     |
| 2    | **AI Agent**        | Pre-scores all entries using embedding similarity and multimodal inspection. |
| 3    | **Human Validator** | Verifies a subset or all entries with score â‰¤ 1.                             |
| 4    | **Jupyter Cell 9**  | Computes averages and correlations with automated metrics.                   |

This design maintains scientific rigor while reducing human annotation time to a fraction of the original effort.

---

Would you like me to prepare the **AI-assisted scoring notebook cell** that automatically estimates these 0â€“2 scores using text-embedding similarity (for caption and VLM evaluation)? It would integrate smoothly with your current CSV workflow.


===
Excellent â€” below is a **complete Jupyter cell** that automatically performs **AI-assisted scoring** of your `accuracy_annotations_multi.csv` file.
It uses **semantic similarity of embeddings** (e.g. from `sentence-transformers`) to estimate 0â€“2 scores for *caption accuracy*, *visualâ€“semantic fidelity*, and *table structure quality*.

This method produces preliminary annotations that you can later review or correct manually.

---

## ğŸ§  AI-Assisted Annotation Scoring Cell

```python
# ============================================================
# 10. AI-Assisted Scoring for Manual Annotation Template
# ============================================================

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
import re

# Load the original annotation CSV
ann_path = "test/accuracy_annotations_multi.csv"
df = pd.read_csv(ann_path)

# Load sentence-transformer model (lightweight, fast)
model = SentenceTransformer("all-MiniLM-L6-v2")

def clean_text(t):
    """Basic cleanup for consistency."""
    if pd.isna(t):
        return ""
    t = re.sub(r"\s+", " ", str(t))
    return t.strip()

# Clean text fields
for col in ["caption", "vlm_description", "content"]:
    df[col] = df[col].apply(clean_text)

# Initialize scoring columns
df["caption_score_ai"] = np.nan
df["vlm_score_ai"] = np.nan
df["table_score_ai"] = np.nan

# Scoring thresholds (can be tuned)
THRESHOLDS = {"high": 0.80, "mid": 0.60}

def score_from_similarity(sim):
    """Map cosine similarity to a discrete 0â€“2 score."""
    if sim >= THRESHOLDS["high"]:
        return 2
    elif sim >= THRESHOLDS["mid"]:
        return 1
    else:
        return 0

print("ğŸ¤– Running AI-assisted scoring...")
for i, row in df.iterrows():
    if row["type"] == "figure":
        # Caption Score: compare extracted caption with VLM description text
        if row["caption"] and row["vlm_description"]:
            emb_cap = model.encode(row["caption"], convert_to_tensor=True)
            emb_vlm = model.encode(row["vlm_description"], convert_to_tensor=True)
            sim = util.cos_sim(emb_cap, emb_vlm).item()
            df.at[i, "caption_score_ai"] = score_from_similarity(sim)
            df.at[i, "vlm_score_ai"] = score_from_similarity(sim)
        else:
            df.at[i, "caption_score_ai"] = 0
            df.at[i, "vlm_score_ai"] = 0
    elif row["type"] == "table":
        # Table Score: structural quality via keyword density heuristic
        text = row["content"]
        if len(text) > 20:
            header_terms = len(re.findall(r"\b(column|row|value|score|id|type|mean)\b", text.lower()))
            numeric_density = len(re.findall(r"\d", text)) / max(len(text), 1)
            score = 2 if header_terms > 1 and numeric_density > 0.05 else (1 if numeric_density > 0.02 else 0)
            df.at[i, "table_score_ai"] = score
        else:
            df.at[i, "table_score_ai"] = 0

print("âœ… AI-assisted scoring complete.")
output_path = "test/accuracy_annotations_scored_ai.csv"
df.to_csv(output_path, index=False)
print(f"ğŸ“„ Saved as: {output_path}")

display(df.head())
```

---

## ğŸ” What This Cell Does

1. **Loads** your annotation file (`accuracy_annotations_multi.csv`).
2. **Cleans** textual fields for consistency.
3. **Computes semantic embeddings** for the caption and the VLM description using `all-MiniLM-L6-v2`.
4. **Calculates cosine similarity**, then maps it to a **0â€“2 discrete score**:

   * â‰¥ 0.80 â†’ **2** (Accurate)
   * 0.60â€“0.79 â†’ **1** (Partially accurate)
   * < 0.60 â†’ **0** (Incorrect)
5. **Estimates table structure quality** based on header keywords and numeric density.
6. **Outputs** a scored file named `accuracy_annotations_scored_ai.csv`.

---

## ğŸ“Š What You Get

The resulting file will include three new columns:

| pdf                  | domain    | type   | caption_score_ai | vlm_score_ai | table_score_ai |
| -------------------- | --------- | ------ | ---------------- | ------------ | -------------- |
| Biology (2023).pdf   | Biology   | figure | 2                | 2            | NaN            |
| Chemistry (2024).pdf | Chemistry | figure | 1                | 1            | NaN            |
| Chemistry (2024).pdf | Chemistry | table  | NaN              | NaN          | 2              |

---

## ğŸ§© Recommended Next Step

After running this cell:

1. Open `accuracy_annotations_scored_ai.csv`.
2. Quickly **filter for low or ambiguous scores** (`== 0 or == 1`).
3. Manually inspect those entries against the original PDF.
4. Then, use the previous **Cell 9** to aggregate human-adjusted and AI-assisted scores together for full multimodal accuracy analysis.

---

Would you like me to extend this pipeline so it **merges AI-assisted scores with your existing human-scored CSV** to produce combined mean metrics per domain (for comparative analysis between human and AI evaluation)?

